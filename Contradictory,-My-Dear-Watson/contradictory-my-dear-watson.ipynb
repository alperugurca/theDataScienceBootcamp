{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contradictory, My Dear Watson\n\nThis project implements a Natural Language Inference (NLI) classifier using XLM-RoBERTa to determine if two given sentences are entailment, neutral, or contradictory. The model is fine-tuned on the \"Contradictory, My Dear Watson\" dataset and deployed as a user-friendly interface using Gradio.\n\nDataset: https://www.kaggle.com/competitions/contradictory-my-dear-watson/data\n\nHugging Face: https://huggingface.co/spaces/alperugurcan/NLI-Classifier","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport pandas as pd\nimport os\nfrom datetime import datetime\n\n# Create timestamp for unique model naming\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_save_path = f'/kaggle/working/nli_model_{timestamp}'\nos.makedirs(model_save_path, exist_ok=True)\n\n# Load data and model\ndata = load_dataset('csv', data_files={\n    'train': '/kaggle/input/contradictory-my-dear-watson/train.csv',\n    'test': '/kaggle/input/contradictory-my-dear-watson/test.csv'\n})\ntrain_val = data['train'].train_test_split(0.2)\n\n# Use xlm-roberta-base\nmodel_name = 'xlm-roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\n# Preprocessing with labels\ndef tokenize(examples, is_test=False):\n    tokenized = tokenizer(\n        examples['premise'],\n        examples['hypothesis'],\n        truncation=True,\n        max_length=64,\n        padding='max_length'\n    )\n    if not is_test:\n        tokenized['labels'] = examples['label']\n    return tokenized\n\n# Process datasets\ntrain_dataset = train_val['train'].map(\n    lambda x: tokenize(x, is_test=False), \n    batched=True, \n    remove_columns=['premise', 'hypothesis', 'id', 'lang_abv', 'language']\n)\nval_dataset = train_val['test'].map(\n    lambda x: tokenize(x, is_test=False), \n    batched=True,\n    remove_columns=['premise', 'hypothesis', 'id', 'lang_abv', 'language']\n)\ntest_dataset = data['test'].map(\n    lambda x: tokenize(x, is_test=True), \n    batched=True,\n    remove_columns=['premise', 'hypothesis', 'id', 'lang_abv', 'language']\n)\n\n# Training configuration\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=model_save_path,\n        per_device_train_batch_size=32,\n        num_train_epochs=2,\n        learning_rate=2e-5,\n        fp16=True,\n        report_to=[],\n        save_strategy='epoch',     # Save at each epoch\n        evaluation_strategy='no',\n        save_total_limit=1,        # Keep only the last model\n    ),\n    train_dataset=train_dataset,\n)\n\n# Train and predict\ntrainer.train()\n\n# Save the final model and tokenizer\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)\n\n# Save model config\nconfig_dict = {\n    'model_name': model_name,\n    'max_length': 64,\n    'num_labels': 3,\n    'training_timestamp': timestamp\n}\nimport json\nwith open(f'{model_save_path}/config.json', 'w') as f:\n    json.dump(config_dict, f)\n\n# Make predictions\npreds = trainer.predict(test_dataset)\n\n# Save predictions\npd.DataFrame({\n    'id': data['test']['id'],\n    'prediction': np.argmax(preds.predictions, axis=1)\n}).to_csv(f'{model_save_path}/submission.csv', index=False)\n\nprint(f\"Model saved to: {model_save_path}\")\n\n# Optional: Save training metrics\nif hasattr(trainer, 'state'):\n    training_stats = {\n        'train_loss': trainer.state.log_history,\n        'best_model_checkpoint': trainer.state.best_model_checkpoint\n    }\n    with open(f'{model_save_path}/training_stats.json', 'w') as f:\n        json.dump(training_stats, f)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T21:47:19.389138Z","iopub.execute_input":"2024-10-30T21:47:19.389724Z","iopub.status.idle":"2024-10-30T21:51:55.609147Z","shell.execute_reply.started":"2024-10-30T21:47:19.389650Z","shell.execute_reply":"2024-10-30T21:51:55.608188Z"}},"execution_count":null,"outputs":[]}]}