{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hugging Face Transformers library to set up a text generation pipeline.\n",
        "\n",
        "Hugging Face: https://huggingface.co/spaces/alperugurcan/text-generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYlsROtK8HIe",
        "outputId": "b432dfd2-ff81-43f5-be39-6112810e5124"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: Once upon a particular place called that time that it was necessary in a certain order, to set out by a certain number of directions, it was needed, in order to come as near to a certain place as possible. And in this order, there\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\")\n",
        "prompt = \"Once upon a\"\n",
        "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "print(f\"Generated text: {generated_text}\");"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
