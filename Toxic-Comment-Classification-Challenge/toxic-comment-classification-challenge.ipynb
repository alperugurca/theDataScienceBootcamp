{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\n\nThis project implements a multi-label text classification model to detect toxic comments. Using TF-IDF vectorization and Logistic Regression, the model identifies six types of toxicity: general toxic, severe toxic, obscene, threat, insult, and identity hate.\n\nDataset: https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data\n\nHugging Face: https://huggingface.co/spaces/alperugurcan/toxic-comment-classifier","metadata":{}},{"cell_type":"code","source":"# 1. Import essential libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport re\n\n# 2. Load data\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n\n# 3. Basic text preprocessing\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\nX = train['comment_text'].apply(clean_text)\ny = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n\n# 4. Improved TF-IDF settings\nvectorizer = TfidfVectorizer(\n    max_features=10000,\n    min_df=3,\n    max_df=0.9,\n    strip_accents='unicode',\n    analyzer='word'\n)\nX_tfidf = vectorizer.fit_transform(X)\nX_test_tfidf = vectorizer.transform(test['comment_text'].apply(clean_text))\n\n# 5. Train model with increased iterations and adjusted solver\nmodel = MultiOutputClassifier(\n    LogisticRegression(\n        max_iter=300,  # Increased from 100\n        solver='saga',  # Changed solver\n        random_state=42\n    )\n)\nmodel.fit(X_tfidf, y)\n\n# 6. Make predictions\npredictions = model.predict_proba(X_test_tfidf)\n\n# 7. Create submission file\nsubmission = pd.DataFrame(columns=['id'] + list(y.columns))\nsubmission['id'] = test['id']\nfor i, column in enumerate(y.columns):\n    submission[column] = [pred[1] for pred in predictions[i]]\n\n# 8. Save submission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T12:53:48.634432Z","iopub.execute_input":"2024-10-31T12:53:48.634986Z","iopub.status.idle":"2024-10-31T12:54:51.653461Z","shell.execute_reply.started":"2024-10-31T12:53:48.634928Z","shell.execute_reply":"2024-10-31T12:54:51.651981Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Save Model and Vectorizer\nimport pickle\n\nwith open('toxic_model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nwith open('toxic_vectorizer.pkl', 'wb') as f:\n    pickle.dump(vectorizer, f)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:29:30.073769Z","iopub.execute_input":"2024-10-31T13:29:30.074216Z","iopub.status.idle":"2024-10-31T13:29:30.187440Z","shell.execute_reply.started":"2024-10-31T13:29:30.074177Z","shell.execute_reply":"2024-10-31T13:29:30.186179Z"},"trusted":true},"execution_count":3,"outputs":[]}]}