{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"profiles.csv\")\n",
    "\n",
    "#####################\n",
    "#####EXPLORATION#####\n",
    "#####################\n",
    "\n",
    "#reading columns\n",
    "print(df.columns)\n",
    "\n",
    "#exploring some of the columns\n",
    "print(df.drinks.unique())\n",
    "print(df.drugs.unique())\n",
    "print(df.education.unique())\n",
    "for i in range(15):\n",
    "    print(df.ethnicity.unique()[i]) #this helped made entries to this column human-readable\n",
    "print(df.income.unique())\n",
    "print(len(df.last_online.unique()))\n",
    "print(df.last_online.head(5)) #given that there's 30k+ entries, I looked at only the first five to see what the data looked like\n",
    "print(df.orientation.unique())\n",
    "print(df.smokes.unique())\n",
    "print(len(df.speaks.unique()))\n",
    "print(df.speaks.head(10)) #given that there's 7k+ entries, I looked at only the first ten to see what the data looked like\n",
    "print(df.status.unique())\n",
    "for i in df.religion.unique():\n",
    "    print(i)\n",
    "for i in df.location.unique():\n",
    "    print(i)\n",
    "\n",
    "#feature engineering: drinks, drugs, smokes\n",
    "df[\"drinks\"] = df[\"drinks\"].map({\"not at all\":0, \"rarely\":1, \"socially\":2, \"often\":3, \"very often\":4, \"desperately\":5})\n",
    "df[\"drugs\"] = df[\"drugs\"].map({\"never\":0, \"sometimes\":1, \"often\":2})\n",
    "df[\"smokes\"] = df[\"smokes\"].map({\"no\":0, \"trying to quit\":1, \"sometimes\":2, \"when drinking\":3, \"yes\":4})\n",
    "print(len(df) - len((df.drinks + df.drugs + df.smokes).dropna())) #when we take out all the entries with NaNs from drinks, drugs, smokes, we remove 17,451 entries (or we have ~70% left)\n",
    "\n",
    "#custom feature: generation\n",
    "generation = []\n",
    "for i in range(len(df)):\n",
    "    if 18 <= df[\"age\"].iloc[i] <= 32:\n",
    "        generation.append(0)\n",
    "    elif 32 < df[\"age\"].iloc[i] <= 47:\n",
    "        generation.append(1)\n",
    "    else:\n",
    "        generation.append(2)\n",
    "df[\"generation\"] = generation\n",
    "\n",
    "#custom feature: languages, based on speaks\n",
    "df[\"languages\"] = df[\"speaks\"].apply(lambda row: 0 if pd.isnull(row) else len(row.split(\", \"))) #converted NaNs to 0 since there's only 50 of them (considered outliers)\n",
    "\n",
    "#custom feature: total length of characters of all essays for each user\n",
    "import math\n",
    "df[\"essay0_length\"] = df[\"essay0\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay1_length\"] = df[\"essay1\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay2_length\"] = df[\"essay2\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay3_length\"] = df[\"essay3\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay4_length\"] = df[\"essay4\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay5_length\"] = df[\"essay5\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay6_length\"] = df[\"essay6\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay7_length\"] = df[\"essay7\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay8_length\"] = df[\"essay8\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"essay9_length\"] = df[\"essay9\"].apply(lambda row: len(row) if type(row) == str else 0)\n",
    "df[\"total_text_length\"] = df.apply(lambda row: row.essay0_length + row.essay1_length + row.essay2_length + row.essay3_length + row.essay4_length + row.essay5_length + row.essay6_length + row.essay7_length + row.essay8_length + row.essay9_length, axis=1)\n",
    "#i used the code below to look at the \"total_text_length\" outlier\n",
    "print(df[\"total_text_length\"].max())\n",
    "#for i in df[df[\"total_text_length\"] == df[\"total_text_length\"].max()][\"essay0\"].tolist():\n",
    "    #print(i)\n",
    "#now, instead of getting rid the outlier, this entry was trimmed to match the next longest essay\n",
    "df[[\"total_text_length\"]].iloc[27528] = 59113\n",
    "\n",
    "#customer feature: religious seriousness\n",
    "religious_seriousness = []\n",
    "for i in range(len(df)):\n",
    "    if type(df[\"religion\"][i]) == str:\n",
    "        if \"laughing\" in df[\"religion\"][i]:\n",
    "            religious_seriousness.append(0)\n",
    "        elif \"not too serious\" in df[\"religion\"][i]:\n",
    "            religious_seriousness.append(1)\n",
    "        elif \"somewhat serious\" in df[\"religion\"][i]:\n",
    "            religious_seriousness.append(2)\n",
    "        elif \"very serious\" in df[\"religion\"][i]:\n",
    "            religious_seriousness.append(4)\n",
    "        else:\n",
    "            religious_seriousness.append(3) #those without the modifying words are considered simplye \"serious\"\n",
    "    else:\n",
    "        religious_seriousness.append(1) #Assumption: NaNs are treated as \"not too serious\" cuz it makes sense that those who didn't even answer this question may not take religion too seriously\n",
    "df[\"religious_seriousness\"] = religious_seriousness\n",
    "\n",
    "#custom feature: indication of how serious profile is about finding love (\"love\" counter in essay9)\n",
    "df[\"love\"] = df[\"essay9\"].apply(lambda row: 0 if pd.isnull(row) else row.casefold().count(\"love\"))\n",
    "\n",
    "#unused custom feature: state, based on location\n",
    "df[\"state\"] = df.apply(lambda row: row[\"location\"].split(\", \")[-1], axis=1)\n",
    "print(df.state.unique())\n",
    "df.state.value_counts() #we'll find out that most people are from CA\n",
    "df[df[\"state\"] == \"california\"][\"location\"].value_counts() #and a majority is from SF\n",
    "\n",
    "#exploring income\n",
    "plt.hist(df.income)\n",
    "plt.show() #since a lot of people didnt share income, this may not be useful if we drop all those that reported \"-1\"\n",
    "\n",
    "#######################\n",
    "#####NORMALIZATION#####\n",
    "#######################\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#cleaning up the data by deleting all the NaNs so each feature would have an even index\n",
    "df = df[[\"age\", \"drinks\", \"smokes\", \"drugs\", \"total_text_length\", \"love\", \"generation\", \"languages\", \"religious_seriousness\"]].dropna()\n",
    "df[\"drinks\"] = scaler.fit_transform(np.reshape(df[[\"drinks\"]], (-1,1)))\n",
    "df[\"smokes\"] = scaler.fit_transform(np.reshape(df[[\"smokes\"]], (-1,1)))\n",
    "df[\"drugs\"] = scaler.fit_transform(np.reshape(df[[\"drugs\"]], (-1,1)))\n",
    "df[\"total_text_length\"] = scaler.fit_transform(np.reshape(df[[\"total_text_length\"]], (-1,1)))\n",
    "df[\"love\"] = scaler.fit_transform(np.reshape(df[[\"love\"]], (-1,1)))\n",
    "df[\"generation\"] = scaler.fit_transform(np.reshape(df[[\"generation\"]], (-1,1)))\n",
    "df[\"languages\"] = scaler.fit_transform(np.reshape(df[[\"languages\"]], (-1,1)))\n",
    "df[\"religious_seriousness\"] = scaler.fit_transform(np.reshape(df[[\"religious_seriousness\"]], (-1,1)))\n",
    "\n",
    "####################################\n",
    "#####MULTIPLE LINEAR REGRESSION#####\n",
    "####################################\n",
    "\n",
    "input(\"\\nMultiple Linear Regression\\nHit any key to continue...\\n\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,y,train_size=0.8,test_size=0.2,random_state = 42) #splitting the data\n",
    "\n",
    "    line_fitter = LinearRegression()\n",
    "    line_fitter.fit(x_train, y_train)\n",
    "\n",
    "    predicted_y = line_fitter.predict(x_train)\n",
    "\n",
    "    score = line_fitter.score(x_test, y_test)\n",
    "    \n",
    "    return score\n",
    "\n",
    "#getitng all combinations of all the features\n",
    "from itertools import combinations\n",
    "features = [\"generation\", \"total_text_length\", \"languages\", \"drinks\", \"drugs\", \"smokes\", \"religious_seriousness\", \"love\"]\n",
    "combo = []\n",
    "for i in range(1,len(features)):\n",
    "    combo.append(list(combinations(features, i)))\n",
    "\n",
    "#saving the scores for every combination of features\n",
    "y = df[[\"age\"]]\n",
    "scores = []\n",
    "for i in range(len(combo)):\n",
    "    for j in range(len(combo[i])):\n",
    "        x = df[[*combo[i][j]]]\n",
    "        scores.append([linear_regression(x, y), combo[i][j]])\n",
    "\n",
    "#printing our scores\n",
    "for i in scores:\n",
    "    print(i)\n",
    "\n",
    "#getting the highest accuracy for our model\n",
    "MLR_best_score = max(scores, key=lambda score: score[0])\n",
    "\n",
    "print(\"\\nOur best score for Multiple Linear Regression is\", MLR_best_score[0], \".\\nThe combination of features used to achieve this is\", MLR_best_score[1], \".\")\n",
    "input(\"Hit any key to continue...\\n\")\n",
    "\n",
    "########################\n",
    "#####KNN REGRESSION#####\n",
    "########################\n",
    "\n",
    "input(\"\\nK-Nearest Neighbors Weighted Regression\\nHit any key to continue...\\n\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def KNN_regression(x, y, neighbors):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,y,train_size=0.8,test_size=0.2,random_state = 42)\n",
    "\n",
    "    regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\")\n",
    "    regressor.fit(x_train, y_train)\n",
    "\n",
    "    score = regressor.score(x_train, y_train)\n",
    "    \n",
    "    return score\n",
    "\n",
    "#saving the scores for every combination of features\n",
    "y = df[[\"age\"]]\n",
    "scores = []\n",
    "for i in range(len(combo)):\n",
    "    for j in range(len(combo[i])):\n",
    "        x = df[[*combo[i][j]]]\n",
    "        scores.append([KNN_regression(x, y, 5), combo[i][j]])\n",
    "\n",
    "#printing our scores\n",
    "for i in scores:\n",
    "    print(i)\n",
    "\n",
    "#getting the highest accuracy for our model\n",
    "KNN_best_score = max(scores, key=lambda score: score[0])\n",
    "\n",
    "print(\"\\nOur best score for KNN Regression is\", KNN_best_score[0], \".\\nThe combination of features used to achieve this is\", KNN_best_score[1], \".\")\n",
    "input(\"Now we'll find out what could be our ideal number of neighbors. Hit any key to continue...\\n\")\n",
    "\n",
    "#finding out ideal neighbors\n",
    "N_score = []\n",
    "for neighbors in range(1,15):\n",
    "        x = df[[*max(scores, key=lambda score: score[0])[1]]]\n",
    "        y = df[[\"age\"]]\n",
    "        N_score.append([KNN_regression(x, y, neighbors), neighbors])\n",
    "\n",
    "print(\"\\nOur best score for KNN Regression is\", max(N_score, key=lambda score: score[0]),\n",
    "      \".\\nWe obtained this by making use of these many neighbors:\", max(N_score, key=lambda score: score[1]), \".\")\n",
    "input(\"Hit any key to continue...\\n\")\n",
    "\n",
    "########################\n",
    "#####KNN CLASSIFIER#####\n",
    "########################\n",
    "\n",
    "input(\"\\nK-Nearest Neighbors Classifier\\nHit any key to continue...\\n\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNNClassifier(x, y, neighbors):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,y,train_size=0.8,test_size=0.2,random_state = 42)\n",
    "    \n",
    "    classifier = KNeighborsClassifier(n_neighbors = neighbors)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    score = classifier.score(x_test, y_test)\n",
    "\n",
    "    return score\n",
    "\n",
    "#updating the features for our classifier\n",
    "features = [\"total_text_length\", \"languages\", \"drinks\", \"drugs\", \"smokes\", \"religious_seriousness\", \"love\"]\n",
    "combo = []\n",
    "for i in range(1,len(features)):\n",
    "    combo.append(list(combinations(features, i)))\n",
    "\n",
    "#saving the scores for every combination of features\n",
    "scores = []\n",
    "y = scaler.inverse_transform(np.reshape(df[[\"generation\"]], (-1,1)))\n",
    "y = np.ravel(y)\n",
    "for i in range(len(combo)):\n",
    "    for j in range(len(combo[i])):\n",
    "        x = df[[*combo[i][j]]]\n",
    "        scores.append([KNNClassifier(x, y, neighbors=5), combo[i][j]])\n",
    "\n",
    "#printing our scores\n",
    "for i in scores:\n",
    "    print(i)\n",
    "\n",
    "#getting the highest accuracy for our model\n",
    "KNNC_best_score = max(scores, key=lambda score: score[0])\n",
    "\n",
    "print(\"\\nOur best score for KNN Classifier is\", KNNC_best_score[0], \".\\nThe combination of features used to achieve this is\", KNNC_best_score[1], \".\")\n",
    "input(\"Now we'll find out what could be our ideal number of neighbors. Hit any key to continue...\\n\")\n",
    "\n",
    "#finding out ideal neighbors\n",
    "N_score = []\n",
    "x = df[[*max(scores, key=lambda score: score[0])[1]]]\n",
    "for neighbors in range(1,15):\n",
    "        N_score.append([KNNClassifier(x, y, neighbors), neighbors])\n",
    "\n",
    "print(\"\\nOur best score for KNN Classifier is\", max(N_score, key=lambda score: score[0]),\n",
    "      \".\\nWe obtained this by making use of these many neighbors:\", max(N_score, key=lambda score: score[1]), \".\")\n",
    "input(\"Hit any key to continue...\\n\")\n",
    "\n",
    "########################\n",
    "#####SVC CLASSIFIER#####\n",
    "########################\n",
    "\n",
    "input(\"\\nSupport Vector Machine Classifier\\nHit any key to continue...\\n\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVCClassifier(x, y, C, gamma):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,y,train_size=0.8,test_size=0.2,random_state = 42)\n",
    "\n",
    "    classifier = SVC(kernel=\"rbf\", C=C, gamma=gamma)\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "    score = classifier.score(x_test, y_test)\n",
    "\n",
    "    return score\n",
    "\n",
    "#finding the best C and gamma using the feature combination that worked best for our KNN Classifier\n",
    "x = df[[*max(scores, key=lambda score: score[0])[1]]] #also equal to df[[\"religious_seriousness\", \"love\"]]\n",
    "y = scaler.inverse_transform(np.reshape(df[[\"generation\"]], (-1,1)))\n",
    "y = np.ravel(y)\n",
    "c = [0.1, 1.0, 10, 100, 1000]\n",
    "gamma = [1, 10, 100]\n",
    "\n",
    "#saving the scores for every combination of features\n",
    "scores = []\n",
    "for C_ in c:\n",
    "    for gamma_ in gamma:\n",
    "        scores.append([SVCClassifier(x, y, C_, gamma_), C_, gamma_])\n",
    "\n",
    "#printing our scores\n",
    "for i in scores:\n",
    "    print(i)\n",
    "\n",
    "#getting the highest accuracy for our model\n",
    "SVC_best_score = max(scores, key=lambda score: score[0])\n",
    "\n",
    "print(\"\\nOur best score for SVC Classifier is\", SVC_best_score[0],\n",
    "      \".\\nWe obtained this by making use of C value of:\", SVC_best_score[1], \"and gamma value of:\",  SVC_best_score[2], \".\")\n",
    "input(\"Hit any key to continue...\\n\")\n",
    "\n",
    "#plotting heat map of diferent C and gamma values and their corresponding accuracies\n",
    "scores_accuracy = [i[0] for i in scores]\n",
    "\n",
    "scores_ = []\n",
    "for i in range(int(len(scores_accuracy)/3)):\n",
    "    scores_.append(scores_accuracy[i*3:(i*3)+3])\n",
    "\n",
    "heat_df = pd.DataFrame(scores_, index=c, columns=gamma)\n",
    "sns.heatmap(heat_df, annot=True, fmt=\"f\", linewidths=0.5, cbar_kws={\"label\": \"ACCURACY\"})\n",
    "\n",
    "plt.xlabel(\"GAMMA\")\n",
    "plt.ylabel(\"C VALUE\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
